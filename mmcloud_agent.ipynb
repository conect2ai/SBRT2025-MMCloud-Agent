{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FZew9Q5WDGf"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqg5JNigWLdr"
      },
      "outputs": [],
      "source": [
        "%pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSBHL6TyXMbU"
      },
      "outputs": [],
      "source": [
        "%pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75asKQEWXwOC"
      },
      "outputs": [],
      "source": [
        "%pip install langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY40PFzcfp3d",
        "outputId": "ffb8c1ff-d55c-47e2-9221-ad95778e5964"
      },
      "outputs": [],
      "source": [
        "%pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 28 key-value pairs and 290 tensors from ./models/Qwen2.5-0.5B.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Models\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 494M\n",
            "llama_model_loader: - kv   4:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-0...\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
            "llama_model_loader: - kv   7:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 896\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 4864\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 14\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type q5_0:  132 tensors\n",
            "llama_model_loader: - type q8_0:   13 tensors\n",
            "llama_model_loader: - type q4_K:   12 tensors\n",
            "llama_model_loader: - type q6_K:   12 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 373.71 MiB (6.35 BPW) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 896\n",
            "print_info: n_layer          = 24\n",
            "print_info: n_head           = 14\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 7\n",
            "print_info: n_embd_k_gqa     = 128\n",
            "print_info: n_embd_v_gqa     = 128\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 4864\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1B\n",
            "print_info: model params     = 494.03 M\n",
            "print_info: general.name     = Models\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =   373.71 MiB\n",
            ".................................................................\n",
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 64\n",
            "llama_init_from_model: n_ubatch      = 8\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
            "llama_init_from_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =     4.69 MiB\n",
            "llama_init_from_model: graph nodes  = 846\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '15', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '896', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Models', 'qwen2.block_count': '24', 'general.type': 'model', 'general.size_label': '494M', 'general.license': 'apache-2.0', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-0.5B/blob/main/LICENSE', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '4864', 'qwen2.attention.head_count': '14', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Passo ===\n",
            "{'check_incidents_node': {'question': 'Acidentes ou multas?', 'context': '⚠️ 370 sinistros próximos detectados. 🚓 7927 multas registradas próximas. ', 'response': '', 'latitude': -5.857612, 'longitude': -35.212723}}\n",
            " Multas ou sinistros próximos detectados no caminho."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    2016.79 ms\n",
            "llama_perf_context_print: prompt eval time =    2016.60 ms /    93 tokens (   21.68 ms per token,    46.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     791.69 ms /    14 runs   (   56.55 ms per token,    17.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    2863.81 ms /   107 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tempo de inferência: 2.87 s\n",
            "Uso de memória (antes/depois): 865.66 MB / 865.66 MB\n",
            "Uso de CPU (antes/depois): 21.00% / 84.10%\n",
            "\n",
            "=== Passo ===\n",
            "{'llm_node': {'question': 'Acidentes ou multas?', 'context': '⚠️ 370 sinistros próximos detectados. 🚓 7927 multas registradas próximas. ', 'response': ' Multas ou sinistros próximos detectados no caminho.', 'latitude': -5.857612, 'longitude': -35.212723}}\n",
            "\n",
            "=== Resposta Final ===\n",
            " Multas ou sinistros próximos detectados no caminho.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import BallTree\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# ======== Estado ========\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    context: str\n",
        "    response: str\n",
        "    latitude: float\n",
        "    longitude: float\n",
        "\n",
        "# ======== Prompt orientado ========\n",
        "def build_prompt(context, question):\n",
        "    q = question.lower()\n",
        "    if \"sinistro\" in q or \"multa\" in q:\n",
        "        example = \"Exemplo: Multas ou sinistros próximos detectados no caminho.\"\n",
        "    else:\n",
        "        example = \"Exemplo de resposta: Você está dirigindo de forma segura ou agressiva.\"\n",
        "    return f\"\"\"Você é um assistente automotivo. Com base no contexto, responda de forma amigável e curta.\n",
        "\n",
        "Contexto: {context}\n",
        "Pergunta: {question}\n",
        "{example}\n",
        "Resposta:\"\"\"\n",
        "\n",
        "# prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# ======== Configuração LlamaCpp ========\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"./models/Qwen2.5-0.5B.Q4_K_M.gguf\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=20,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# ======== Carregar datasets de acidentes e multas ========\n",
        "acidentes = pd.read_csv('acidentes_processado.csv')\n",
        "multas = pd.read_csv('multas_processado.csv')\n",
        "acidentes_coords = np.radians(acidentes[['latitude', 'longitude']].values)\n",
        "multas_coords = np.radians(multas[['latitude', 'longitude']].values)\n",
        "acidentes_tree = BallTree(acidentes_coords, metric='haversine')\n",
        "multas_tree = BallTree(multas_coords, metric='haversine')\n",
        "RAIO_METROS = 500\n",
        "RAIO_RADIANOS = RAIO_METROS / 6371000\n",
        "\n",
        "# ======== Tool: comportamento do motorista ========\n",
        "def get_last_driver_behavior():\n",
        "    try:\n",
        "        data = pd.read_csv(\"obd_data.csv\")\n",
        "        if 'driver_behavior' not in data.columns:\n",
        "            return \"Não foi possível encontrar a coluna driver_behavior.\"\n",
        "        last_value = data['driver_behavior'].iloc[-1]\n",
        "        behavior_map = {\n",
        "            \"cautious\": \"Você está dirigindo de forma cautelosa. Ótimo para segurança e economia!\",\n",
        "            \"normal\": \"Você está dirigindo de forma normal. Continue atento à estrada.\",\n",
        "            \"aggressive\": \"Você está dirigindo de forma agressiva. Recomendo reduzir a velocidade e dirigir com mais cuidado.\"\n",
        "        }\n",
        "        return behavior_map.get(last_value.lower(), \"Comportamento desconhecido.\")\n",
        "    except Exception as e:\n",
        "        return f\"Erro ao ler o arquivo: {e}\"\n",
        "\n",
        "def tool_node(state: AgentState):\n",
        "    summary = get_last_driver_behavior()\n",
        "    state[\"context\"] = summary\n",
        "    return state\n",
        "\n",
        "# ======== Tool: acidentes e multas ========\n",
        "def check_incidents_node(state: AgentState):\n",
        "    latitude_atual = state.get('latitude', -5.7945)\n",
        "    longitude_atual = state.get('longitude', -35.211)\n",
        "    coord_atual = np.radians([[latitude_atual, longitude_atual]])\n",
        "\n",
        "    acidentes_idx = acidentes_tree.query_radius(coord_atual, r=RAIO_RADIANOS)[0]\n",
        "    multas_idx = multas_tree.query_radius(coord_atual, r=RAIO_RADIANOS)[0]\n",
        "\n",
        "    summary = \"\"\n",
        "    if len(acidentes_idx) > 0:\n",
        "        summary += f\"⚠️ {len(acidentes_idx)} sinistros próximos detectados. \"\n",
        "    if len(multas_idx) > 0:\n",
        "        summary += f\"🚓 {len(multas_idx)} multas registradas próximas. \"\n",
        "    if summary == \"\":\n",
        "        summary = \"✅ Nenhum sinistro ou multa próximo.\"\n",
        "\n",
        "    state[\"context\"] = summary\n",
        "    return state\n",
        "\n",
        "# ======== LLM Node ========\n",
        "def llm_node(state: AgentState):\n",
        "    if state.get(\"response\"):\n",
        "        return state\n",
        "\n",
        "    q = state[\"question\"]\n",
        "    context = state.get(\"context\", \"\")\n",
        "    final_prompt = build_prompt(context, q)\n",
        "\n",
        "    # ⏱ medir tempo\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # 📊 medir CPU/memória antes\n",
        "    process = psutil.Process()\n",
        "    cpu_before = psutil.cpu_percent(interval=None)\n",
        "    mem_before = process.memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    result = llm.invoke(final_prompt)\n",
        "\n",
        "    # ⏱ medir tempo final\n",
        "    end_time = time.perf_counter()\n",
        "    elapsed = end_time - start_time\n",
        "\n",
        "    # 📊 medir CPU/memória depois\n",
        "    cpu_after = psutil.cpu_percent(interval=None)\n",
        "    mem_after = process.memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    state[\"response\"] = result\n",
        "\n",
        "    # Exibir resultados\n",
        "    print(f\"Tempo de inferência: {elapsed:.2f} s\")\n",
        "    print(f\"Uso de memória (antes/depois): {mem_before:.2f} MB / {mem_after:.2f} MB\")\n",
        "    print(f\"Uso de CPU (antes/depois): {cpu_before:.2f}% / {cpu_after:.2f}%\")\n",
        "\n",
        "    # Se o modelo fornecer token count → calcular taxa\n",
        "    if hasattr(llm, 'n_tokens'):\n",
        "        n_tokens = llm.n_tokens\n",
        "        tokens_per_sec = n_tokens / elapsed\n",
        "        print(f\"Tokens gerados: {n_tokens}, Taxa: {tokens_per_sec:.2f} tokens/s\")\n",
        "\n",
        "    return state\n",
        "\n",
        "# ======== Roteador ========\n",
        "def route_node(state: AgentState):\n",
        "    q = state[\"question\"].lower()\n",
        "    if \"acidente\" in q or \"multa\" in q:\n",
        "        return \"check_incidents_node\"\n",
        "    elif \"como estou dirigindo\" in q or \"comportamento\" in q:\n",
        "        return \"tool_node\"\n",
        "    else:\n",
        "        return \"tool_node\"  # fallback\n",
        "\n",
        "# ======== Grafo corrigido ========\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"tool_node\", tool_node)\n",
        "graph.add_node(\"check_incidents_node\", check_incidents_node)\n",
        "graph.add_node(\"llm_node\", llm_node)\n",
        "\n",
        "# Aqui entra a correção:\n",
        "graph.add_conditional_edges(START, route_node, {\n",
        "    \"check_incidents_node\": \"check_incidents_node\",\n",
        "    \"tool_node\": \"tool_node\"\n",
        "})\n",
        "\n",
        "graph.add_edge(\"tool_node\", \"llm_node\")\n",
        "graph.add_edge(\"check_incidents_node\", \"llm_node\")\n",
        "graph.add_edge(\"llm_node\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "# ======== Exemplo de execução ========\n",
        "initial_state = AgentState(\n",
        "    question=\"Acidentes ou multas?\",\n",
        "    context=\"\",\n",
        "    response=\"\",\n",
        "    latitude=-5.857612,    # exemplo real de Natal\n",
        "    longitude=-35.212723\n",
        ")\n",
        "\n",
        "for step in app.stream(initial_state):\n",
        "    print(\"\\n=== Passo ===\")\n",
        "    print(step)\n",
        "\n",
        "print(\"\\n=== Resposta Final ===\")\n",
        "print(step[\"llm_node\"][\"response\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyttsx3\n",
        "\n",
        "# Inicializa o mecanismo de TTS\n",
        "engine = pyttsx3.init()\n",
        "\n",
        "# Define o texto que será falado\n",
        "# texto = \"Você está dirigindo de forma muito agressiva. Por favor, reduza a velocidade.\"\n",
        "\n",
        "texto = step[\"llm_node\"][\"response\"]\n",
        "# Configurações opcionais\n",
        "# Alterar a velocidade da fala (padrão: 200)\n",
        "engine.setProperty('rate', 100)\n",
        "\n",
        "# Alterar o volume (0.0 a 1.0)\n",
        "engine.setProperty('volume', 0.5)\n",
        "\n",
        "# Alterar a voz (masculina ou feminina, depende das vozes disponíveis no sistema)\n",
        "voices = engine.getProperty('voices')\n",
        "engine.setProperty('voice', voices[0].id)  # 0 para masculina, 1 para feminina\n",
        "\n",
        "# Fala o texto\n",
        "engine.say(texto)\n",
        "\n",
        "# Aguarda a conclusão da fala\n",
        "engine.runAndWait()\n",
        "\n",
        "engine.save_to_file(texto, \"saida.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noaH5N-5iEQQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
